# SQLite to PostgreSQL Migration Plan (v2.0)
## Enterprise-Grade Migration with Zero Data Loss Guarantee

---

## Executive Summary

**Problem:** SQLite database files are included in deployment snapshots. Every "Republish" replaces production's database with the dev snapshot, causing **catastrophic data loss** of all orders, shipments, and inventory updates processed since the last deployment.

**Solution:** Migrate to Replit's PostgreSQL database, which persists independently of deployments and provides true production data continuity.

**Effort:** **25-30 hours** (revised realistic estimate)
**Risk Level:** Medium-High (mitigated with comprehensive testing and rollback automation)
**Business Impact:** **CRITICAL** - Required for production reliability

**Key Improvements in v2.0:**
- ‚úÖ Data freeze procedures to prevent data loss
- ‚úÖ Automated rollback scripts (tested)
- ‚úÖ Complete code change inventory (15+ files)
- ‚úÖ Replit deployment configuration steps
- ‚úÖ Dual-database validation testing
- ‚úÖ Post-migration monitoring plan
- ‚úÖ Realistic timeline with contingency buffer

---

## Table of Contents

1. [Migration Overview](#migration-overview)
2. [Pre-Migration Preparation](#pre-migration-preparation)
3. [Phase 1: Environment & Dependencies](#phase-1-environment--dependencies)
4. [Phase 2: PostgreSQL Setup](#phase-2-postgresql-setup)
5. [Phase 3: Schema Migration](#phase-3-schema-migration)
6. [Phase 4: Code Migration](#phase-4-code-migration)
7. [Phase 5: Data Freeze & Migration](#phase-5-data-freeze--migration)
8. [Phase 6: Comprehensive Testing](#phase-6-comprehensive-testing)
9. [Phase 7: Deployment Configuration](#phase-7-deployment-configuration)
10. [Phase 8: Production Cutover](#phase-8-production-cutover)
11. [Phase 9: Post-Migration Monitoring](#phase-9-post-migration-monitoring)
12. [Automated Rollback Procedures](#automated-rollback-procedures)
13. [Troubleshooting Guide](#troubleshooting-guide)

---

## Migration Overview

### Current Architecture (SQLite - BROKEN)

```
Development Workspace:
  ‚îú‚îÄ‚îÄ ora.db (SQLite file)
  ‚îî‚îÄ‚îÄ Python scripts ‚Üí Read/Write to ora.db

Production Deployment:
  ‚îú‚îÄ‚îÄ ora.db (SNAPSHOT from publish time)
  ‚îî‚îÄ‚îÄ Python scripts ‚Üí Read/Write to SNAPSHOT
  
‚ö†Ô∏è PROBLEM: On republish ‚Üí Production ora.db is REPLACED with dev snapshot
           ‚Üí ALL production data LOST (orders, shipments, inventory)
```

### Target Architecture (PostgreSQL - PERSISTENT)

```
Development Workspace:
  ‚îî‚îÄ‚îÄ Python scripts ‚Üí PostgreSQL (production database)

Production Deployment:
  ‚îî‚îÄ‚îÄ Python scripts ‚Üí PostgreSQL (same production database)
  
‚úÖ SOLUTION: Single persistent database used by both dev and prod
           ‚Üí ZERO data loss on republish
           ‚Üí Shared database eliminates sync issues
```

### Migration Strategy: Phased with Safety Checkpoints

**Phase Approach:**
1. **Prepare** - Dependencies, backups, rollback automation
2. **Build** - PostgreSQL setup, schema, code changes
3. **Freeze** - Stop all writes to SQLite
4. **Migrate** - One-time data transfer with validation
5. **Test** - Comprehensive validation before cutover
6. **Deploy** - Staged rollout with monitoring
7. **Monitor** - 7-day intensive observation period

**Safety Checkpoints:**
- ‚úÖ GO/NO-GO decision before data freeze
- ‚úÖ Automated rollback if any test fails
- ‚úÖ Canary deployment (1 workflow first)
- ‚úÖ Abort criteria at each phase

---

## Pre-Migration Preparation

### 1. Create Migration Workspace

```bash
# Create comprehensive migration structure
mkdir -p migration/{backups,scripts,logs,rollback,validation}
mkdir -p migration/scripts/{postgres,validation,rollback}

# Document current state
date > migration/migration_started.txt
git rev-parse HEAD > migration/git_commit.txt
```

### 2. Backup Everything (Critical!)

```bash
# 1. SQLite database backup
cp ora.db migration/backups/ora_pre_postgres_$(date +%Y%m%d_%H%M%S).db

# 2. Download backup from Replit (redundancy)
# In Replit UI: Click ora.db ‚Üí Download

# 3. Export complete schema
sqlite3 ora.db ".schema" > migration/backups/sqlite_schema.sql

# 4. Export all data as SQL
sqlite3 ora.db ".dump" > migration/backups/sqlite_full_dump.sql

# 5. Document row counts
sqlite3 ora.db <<EOF > migration/backups/row_counts.txt
.mode column
.headers on
SELECT 'shipped_orders' as table_name, COUNT(*) as rows FROM shipped_orders
UNION ALL SELECT 'shipped_items', COUNT(*) FROM shipped_items
UNION ALL SELECT 'orders_inbox', COUNT(*) FROM orders_inbox
UNION ALL SELECT 'inventory_current', COUNT(*) FROM inventory_current
UNION ALL SELECT 'inventory_transactions', COUNT(*) FROM inventory_transactions
UNION ALL SELECT 'bundle_skus', COUNT(*) FROM bundle_skus
UNION ALL SELECT 'bundle_components', COUNT(*) FROM bundle_components
UNION ALL SELECT 'sku_lot', COUNT(*) FROM sku_lot
UNION ALL SELECT 'workflow_controls', COUNT(*) FROM workflow_controls
UNION ALL SELECT 'shipping_violations', COUNT(*) FROM shipping_violations
UNION ALL SELECT 'configuration_params', COUNT(*) FROM configuration_params
UNION ALL SELECT 'system_kpis', COUNT(*) FROM system_kpis;
EOF

# 6. Backup current codebase
git add -A
git commit -m "PRE-MIGRATION SNAPSHOT: SQLite codebase before PostgreSQL migration"
git tag pre-postgres-migration-$(date +%Y%m%d)
```

**Verification Checklist:**
- [ ] SQLite backup file exists and is readable
- [ ] Downloaded backup stored safely off-Replit
- [ ] Schema export complete
- [ ] Full dump export complete
- [ ] Row counts documented
- [ ] Git commit created and tagged
- [ ] All backups verified with test restore

### 3. Code Change Inventory

**Complete list of files requiring changes:**

**Database Layer (Critical):**
1. `src/services/database/db_utils.py` - Complete rewrite for PostgreSQL
2. Create `src/services/database/pg_utils.py` - New PostgreSQL utilities

**Automation Scripts (15 files):**
3. `src/unified_shipstation_sync.py` - SQL syntax, placeholders, imports
4. `src/scheduled_xml_import.py` - SQL syntax, placeholders
5. `src/scheduled_shipstation_upload.py` - SQL syntax, placeholders
6. `src/scheduled_cleanup.py` - SQL syntax, placeholders
7. `src/weekly_reporter.py` - SQL syntax, placeholders
8. `src/shipstation_units_refresher.py` - SQL syntax, placeholders
9. `app.py` - Flask dashboard, SQL syntax, imports
10. Any scripts in `src/` using database

**Configuration:**
11. `config/settings.py` - Add PostgreSQL configuration
12. `requirements.txt` - Add psycopg2-binary
13. `.replit` - Update run command if needed
14. `replit.nix` - Add PostgreSQL client tools

**Deployment:**
15. Create `scripts/deploy_postgres.sh` - Deployment automation
16. Create `scripts/rollback_to_sqlite.sh` - Rollback automation

**SQL Changes Required:**
- ‚úÖ `?` placeholders ‚Üí `%s` placeholders (ALL SQL queries)
- ‚úÖ `BEGIN IMMEDIATE` ‚Üí `BEGIN` (PostgreSQL default)
- ‚úÖ `AUTOINCREMENT` ‚Üí `SERIAL` (schema only)
- ‚úÖ `DATETIME('now')` ‚Üí `CURRENT_TIMESTAMP`
- ‚úÖ `DATE('now')` ‚Üí `CURRENT_DATE`
- ‚úÖ `STRFTIME(...)` ‚Üí `TO_CHAR(...)`
- ‚úÖ `sqlite3` exceptions ‚Üí `psycopg2` exceptions
- ‚úÖ Remove all `PRAGMA` statements

**Estimated Effort Breakdown:**
- Database layer rewrite: 6-8 hours
- Scripts conversion (15 files): 8-10 hours
- Testing & validation: 6-8 hours
- Deployment config: 2-3 hours
- Rollback automation: 2-3 hours
- Buffer for issues: +5 hours
- **Total: 29-37 hours**

### 4. Install Dependencies

```bash
# Install PostgreSQL adapter
pip install psycopg2-binary

# Update requirements.txt
echo "psycopg2-binary==2.9.9" >> requirements.txt

# Verify installation
python -c "import psycopg2; print('‚úÖ PostgreSQL driver installed')"
```

---

## Phase 1: Environment & Dependencies

### 1.1 Update Replit Configuration

**Update `replit.nix`:**
```nix
# Add PostgreSQL client tools
{ pkgs }: {
  deps = [
    pkgs.python311
    pkgs.postgresql
    pkgs.python311Packages.psycopg2
  ];
}
```

**Update `requirements.txt`:**
```txt
# Add to requirements.txt
psycopg2-binary==2.9.9
pg8000==1.30.3  # Backup pure-Python driver
```

### 1.2 Create Rollback Automation (BEFORE Migration)

**Critical: Build rollback automation FIRST**

```bash
# migration/scripts/rollback/01_rollback_to_sqlite.sh
#!/bin/bash
set -e

echo "üîÑ EMERGENCY ROLLBACK TO SQLITE"
echo "================================"

# 1. Stop all workflows
echo "‚è∏Ô∏è  Stopping all workflows..."
psql $DATABASE_URL -c "UPDATE workflow_controls SET enabled = FALSE;"

# 2. Restore SQLite backup
echo "üì¶ Restoring SQLite backup..."
BACKUP=$(ls -t migration/backups/ora_pre_postgres_*.db | head -1)
cp "$BACKUP" ora.db
echo "‚úÖ Restored: $BACKUP"

# 3. Revert code to SQLite version
echo "‚èÆÔ∏è  Reverting code to pre-migration state..."
git checkout pre-postgres-migration-*

# 4. Restore SQLite configuration
echo "üîß Restoring SQLite configuration..."
export USE_POSTGRES=false
unset DATABASE_URL

# 5. Restart workflows with SQLite
echo "‚ñ∂Ô∏è  Restarting workflows..."
python scripts/restart_workflows.sh

# 6. Verify rollback
echo "‚úÖ Verifying rollback..."
sqlite3 ora.db "SELECT COUNT(*) FROM shipped_orders;"

echo "================================"
echo "‚úÖ ROLLBACK COMPLETE"
echo "System restored to SQLite"
```

**Make executable:**
```bash
chmod +x migration/scripts/rollback/01_rollback_to_sqlite.sh

# TEST rollback script (dry-run)
bash -n migration/scripts/rollback/01_rollback_to_sqlite.sh
echo "‚úÖ Rollback script syntax valid"
```

### 1.3 Create Deployment Automation

```bash
# migration/scripts/deploy_postgres.sh
#!/bin/bash
set -e

echo "üöÄ POSTGRESQL DEPLOYMENT"
echo "========================"

# 1. Verify DATABASE_URL exists
if [ -z "$DATABASE_URL" ]; then
    echo "‚ùå ERROR: DATABASE_URL not set"
    exit 1
fi

# 2. Test PostgreSQL connection
psql $DATABASE_URL -c "SELECT version();" || {
    echo "‚ùå ERROR: Cannot connect to PostgreSQL"
    exit 1
}

# 3. Update environment
export USE_POSTGRES=true

# 4. Run database migrations
python migration/scripts/postgres/01_create_schema.py

# 5. Migrate data
python migration/scripts/postgres/02_migrate_data.py

# 6. Validate migration
python migration/scripts/postgres/03_verify_migration.py || {
    echo "‚ùå VALIDATION FAILED - Aborting"
    exit 1
}

# 7. Deploy to Replit
# (Manual: Click "Republish" in Replit UI)

echo "========================"
echo "‚úÖ DEPLOYMENT READY"
```

---

## Phase 2: PostgreSQL Setup

### 2.1 Create Replit PostgreSQL Database

**Manual Steps in Replit:**
1. Open Replit project
2. Click "Tools" ‚Üí "Database"  
3. Select "PostgreSQL"
4. Click "Create Database"
5. Wait ~2 minutes for provisioning
6. Verify environment variables created

**Auto-created Environment Variables:**
- `DATABASE_URL` - Connection string
- `PGHOST` - Host
- `PGPORT` - Port (usually 5432)
- `PGUSER` - Username
- `PGPASSWORD` - Password
- `PGDATABASE` - Database name

### 2.2 Verify PostgreSQL Connection

```python
# migration/scripts/postgres/00_test_connection.py
import os
import psycopg2

def test_connection():
    try:
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        cur.execute('SELECT version();')
        version = cur.fetchone()
        print(f"‚úÖ PostgreSQL connected: {version[0]}")
        
        # Test write permissions
        cur.execute('CREATE TABLE IF NOT EXISTS test_table (id SERIAL PRIMARY KEY);')
        cur.execute('DROP TABLE test_table;')
        conn.commit()
        print("‚úÖ Write permissions verified")
        
        cur.close()
        conn.close()
        return True
    except Exception as e:
        print(f"‚ùå Connection failed: {e}")
        return False

if __name__ == '__main__':
    success = test_connection()
    exit(0 if success else 1)
```

**Run test:**
```bash
python migration/scripts/postgres/00_test_connection.py
```

---

## Phase 3: Schema Migration

### 3.1 PostgreSQL Schema (Complete)

```sql
-- migration/scripts/postgres/schema.sql

-- Enable extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- 1. Configuration Parameters
CREATE TABLE configuration_params (
    id SERIAL PRIMARY KEY,
    category VARCHAR(50) NOT NULL,
    parameter_name VARCHAR(100) NOT NULL,
    value TEXT,
    sku VARCHAR(50),
    notes TEXT,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(category, parameter_name, sku)
);

-- 2. Workflow Controls
CREATE TABLE workflow_controls (
    workflow_name VARCHAR(100) PRIMARY KEY,
    enabled BOOLEAN NOT NULL DEFAULT TRUE,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by VARCHAR(50)
);

-- 3. Bundle SKUs
CREATE TABLE bundle_skus (
    bundle_sku VARCHAR(50) PRIMARY KEY,
    bundle_name VARCHAR(200),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 4. Bundle Components
CREATE TABLE bundle_components (
    id SERIAL PRIMARY KEY,
    bundle_sku VARCHAR(50) NOT NULL,
    component_sku VARCHAR(50) NOT NULL,
    quantity INTEGER NOT NULL CHECK (quantity > 0),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (bundle_sku) REFERENCES bundle_skus(bundle_sku) ON DELETE CASCADE,
    UNIQUE(bundle_sku, component_sku)
);

-- 5. SKU Lot
CREATE TABLE sku_lot (
    id SERIAL PRIMARY KEY,
    sku VARCHAR(50) NOT NULL,
    lot VARCHAR(50) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(sku, lot)
);

-- 6. Orders Inbox
CREATE TABLE orders_inbox (
    id SERIAL PRIMARY KEY,
    order_number VARCHAR(50) UNIQUE NOT NULL,
    order_date TIMESTAMP,
    customer_name VARCHAR(200),
    shipping_method VARCHAR(100),
    total_amount INTEGER,
    items_json TEXT,
    source VARCHAR(50) DEFAULT 'XML',
    import_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    uploaded_to_shipstation BOOLEAN DEFAULT FALSE,
    upload_date TIMESTAMP,
    shipstation_order_id VARCHAR(50),
    status VARCHAR(50) DEFAULT 'pending'
);

-- 7. Shipped Orders
CREATE TABLE shipped_orders (
    order_number VARCHAR(50) PRIMARY KEY,
    order_date TIMESTAMP,
    ship_date TIMESTAMP,
    customer_name VARCHAR(200),
    carrier VARCHAR(100),
    service VARCHAR(100),
    tracking_number VARCHAR(100),
    shipstation_order_id VARCHAR(50),
    warehouse VARCHAR(100),
    import_source VARCHAR(50) DEFAULT 'ShipStation',
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 8. Shipped Items
CREATE TABLE shipped_items (
    id SERIAL PRIMARY KEY,
    order_number VARCHAR(50) NOT NULL,
    sku VARCHAR(50) NOT NULL,
    lot_number VARCHAR(50),
    quantity INTEGER NOT NULL CHECK (quantity > 0),
    ship_date TIMESTAMP,
    FOREIGN KEY (order_number) REFERENCES shipped_orders(order_number) ON DELETE CASCADE
);

-- 9. Inventory Current
CREATE TABLE inventory_current (
    sku VARCHAR(50) PRIMARY KEY,
    quantity_on_hand INTEGER DEFAULT 0,
    reorder_point INTEGER DEFAULT 0,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 10. Inventory Transactions
CREATE TABLE inventory_transactions (
    id SERIAL PRIMARY KEY,
    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    sku VARCHAR(50) NOT NULL,
    transaction_type VARCHAR(50) NOT NULL,
    quantity INTEGER NOT NULL,
    lot_number VARCHAR(50),
    reference_order VARCHAR(50),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 11. System KPIs
CREATE TABLE system_kpis (
    metric_name VARCHAR(100) PRIMARY KEY,
    metric_value TEXT,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 12. Shipping Violations
CREATE TABLE shipping_violations (
    id SERIAL PRIMARY KEY,
    order_number VARCHAR(50),
    violation_type VARCHAR(100),
    expected_value VARCHAR(200),
    actual_value VARCHAR(200),
    severity VARCHAR(20) DEFAULT 'medium',
    detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    resolved_at TIMESTAMP,
    resolved_by VARCHAR(100),
    notes TEXT,
    FOREIGN KEY (order_number) REFERENCES shipped_orders(order_number) ON DELETE CASCADE
);

-- Performance Indexes
CREATE INDEX idx_shipped_items_order ON shipped_items(order_number);
CREATE INDEX idx_shipped_items_sku ON shipped_items(sku);
CREATE INDEX idx_shipped_items_ship_date ON shipped_items(ship_date);
CREATE INDEX idx_shipped_orders_ship_date ON shipped_orders(ship_date);
CREATE INDEX idx_shipped_orders_updated ON shipped_orders(last_updated);
CREATE INDEX idx_orders_inbox_status ON orders_inbox(status);
CREATE INDEX idx_orders_inbox_upload ON orders_inbox(uploaded_to_shipstation);
CREATE INDEX idx_bundle_components_bundle ON bundle_components(bundle_sku);
CREATE INDEX idx_inventory_transactions_sku ON inventory_transactions(sku);
CREATE INDEX idx_inventory_transactions_date ON inventory_transactions(transaction_date);
CREATE INDEX idx_shipping_violations_order ON shipping_violations(order_number);
CREATE INDEX idx_shipping_violations_resolved ON shipping_violations(resolved_at);
CREATE INDEX idx_sku_lot_active ON sku_lot(is_active) WHERE is_active = TRUE;
```

### 3.2 Execute Schema Creation

```bash
# Run schema creation
psql $DATABASE_URL -f migration/scripts/postgres/schema.sql

# Verify tables created
psql $DATABASE_URL -c "\dt"

# Verify indexes created
psql $DATABASE_URL -c "\di"
```

---

## Phase 4: Code Migration

### 4.1 Create PostgreSQL Database Utilities

```python
# src/services/database/pg_utils.py
"""PostgreSQL database utilities with connection pooling"""

import os
import psycopg2
from psycopg2.extras import RealDictCursor
from psycopg2 import pool
from contextlib import contextmanager
from typing import List, Dict, Any, Optional
import logging
import time

logger = logging.getLogger(__name__)

# Connection pool
_connection_pool: Optional[pool.SimpleConnectionPool] = None

def get_pool():
    """Get or create connection pool"""
    global _connection_pool
    if _connection_pool is None:
        _connection_pool = pool.SimpleConnectionPool(
            minconn=2,
            maxconn=20,
            dsn=os.environ.get('DATABASE_URL')
        )
    return _connection_pool

def get_connection():
    """Get connection from pool"""
    return get_pool().getconn()

def return_connection(conn):
    """Return connection to pool"""
    get_pool().putconn(conn)

@contextmanager
def transaction():
    """Transaction context manager"""
    conn = get_connection()
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        return_connection(conn)

def execute_query(sql: str, params: tuple = ()) -> List[Dict[str, Any]]:
    """Execute SELECT query and return list of dicts"""
    with transaction() as conn:
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        cursor.execute(sql, params)
        return [dict(row) for row in cursor.fetchall()]

def execute_update(sql: str, params: tuple = ()) -> int:
    """Execute INSERT/UPDATE/DELETE and return affected rows"""
    with transaction() as conn:
        cursor = conn.cursor()
        cursor.execute(sql, params)
        return cursor.rowcount

def upsert(table: str, data: dict, conflict_columns: list):
    """PostgreSQL UPSERT using ON CONFLICT"""
    columns = list(data.keys())
    values = list(data.values())
    placeholders = ','.join(['%s'] * len(columns))
    conflict = ','.join(conflict_columns)
    
    # Build update clause (exclude conflict columns)
    update_cols = [k for k in columns if k not in conflict_columns]
    update_clause = ','.join([f"{k}=EXCLUDED.{k}" for k in update_cols])
    
    sql = f"""
        INSERT INTO {table} ({','.join(columns)})
        VALUES ({placeholders})
        ON CONFLICT ({conflict}) DO UPDATE SET {update_clause}
        RETURNING *
    """
    
    with transaction() as conn:
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        cursor.execute(sql, tuple(values))
        return dict(cursor.fetchone()) if cursor.rowcount > 0 else None

# Workflow control with caching
_workflow_cache = {}
_cache_ttl = {}

def is_workflow_enabled(workflow_name: str, cache_ttl: int = 45) -> bool:
    """Check if workflow is enabled (with caching)"""
    current_time = time.time()
    
    # Check cache
    if workflow_name in _workflow_cache:
        if current_time - _cache_ttl[workflow_name] < cache_ttl:
            return _workflow_cache[workflow_name]
    
    # Query database
    try:
        result = execute_query(
            "SELECT enabled FROM workflow_controls WHERE workflow_name = %s",
            (workflow_name,)
        )
        enabled = result[0]['enabled'] if result else True  # Fail-open
        
        # Update cache
        _workflow_cache[workflow_name] = enabled
        _cache_ttl[workflow_name] = current_time
        
        return enabled
    except Exception as e:
        logger.error(f"Error checking workflow status: {e}")
        return _workflow_cache.get(workflow_name, True)

def update_workflow_status(workflow_name: str, enabled: bool, updated_by: str = 'system'):
    """Update workflow enabled status"""
    sql = """
        INSERT INTO workflow_controls (workflow_name, enabled, updated_by, last_updated)
        VALUES (%s, %s, %s, CURRENT_TIMESTAMP)
        ON CONFLICT (workflow_name) DO UPDATE SET
            enabled = EXCLUDED.enabled,
            updated_by = EXCLUDED.updated_by,
            last_updated = CURRENT_TIMESTAMP
    """
    execute_update(sql, (workflow_name, enabled, updated_by))
    
    # Clear cache
    if workflow_name in _workflow_cache:
        del _workflow_cache[workflow_name]
        del _cache_ttl[workflow_name]

def close_pool():
    """Close connection pool (for cleanup)"""
    global _connection_pool
    if _connection_pool is not None:
        _connection_pool.closeall()
        _connection_pool = None
```

### 4.2 Update Configuration

```python
# config/settings.py
import os

# Database configuration
USE_POSTGRES = os.environ.get('USE_POSTGRES', 'false').lower() == 'true'

if USE_POSTGRES:
    DATABASE_URL = os.environ.get('DATABASE_URL')
    if not DATABASE_URL:
        raise ValueError("USE_POSTGRES=true but DATABASE_URL not set!")
    print("‚úÖ Using PostgreSQL database")
else:
    SQLITE_PATH = os.environ.get('DATABASE_PATH', 'ora.db')
    print(f"‚úÖ Using SQLite database: {SQLITE_PATH}")
```

### 4.3 Automated SQL Syntax Converter

```python
# migration/scripts/convert_sql_syntax.py
"""Automated SQLite ‚Üí PostgreSQL SQL syntax converter"""

import re
import sys
from pathlib import Path

def convert_placeholders(content: str) -> str:
    """Convert ? placeholders to %s"""
    # Match SQL queries with ? placeholders
    # This is a simple replace - may need manual review
    return content.replace('?', '%s')

def convert_datetime_functions(content: str) -> str:
    """Convert SQLite datetime functions to PostgreSQL"""
    replacements = {
        r"DATETIME\('now'\)": "CURRENT_TIMESTAMP",
        r"DATE\('now'\)": "CURRENT_DATE",
        r"DATE\('now',\s*'([+-]\d+)\s+days?'\)": r"CURRENT_DATE + INTERVAL '\1 days'",
    }
    
    for pattern, replacement in replacements.items():
        content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
    
    return content

def convert_imports(content: str) -> str:
    """Convert imports from sqlite3 to psycopg2"""
    # Check if file uses database
    if 'from src.services.database.db_utils import' in content:
        # Change to pg_utils
        content = content.replace(
            'from src.services.database.db_utils import',
            'from src.services.database.pg_utils import'
        )
    
    return content

def remove_pragma(content: str) -> str:
    """Remove SQLite PRAGMA statements"""
    content = re.sub(r'PRAGMA\s+\w+\s*=?\s*\w*;?', '', content, flags=re.IGNORECASE)
    return content

def convert_file(filepath: Path, dry_run: bool = True):
    """Convert a single file"""
    print(f"\n{'[DRY RUN] ' if dry_run else ''}Converting: {filepath}")
    
    with open(filepath, 'r') as f:
        original = f.read()
    
    # Apply conversions
    converted = original
    converted = convert_placeholders(converted)
    converted = convert_datetime_functions(converted)
    converted = convert_imports(converted)
    converted = remove_pragma(converted)
    
    # Show changes
    if original != converted:
        print(f"  ‚úÖ Changes detected")
        if not dry_run:
            with open(filepath, 'w') as f:
                f.write(converted)
            print(f"  üíæ File updated")
    else:
        print(f"  ‚è≠Ô∏è  No changes needed")

def main():
    dry_run = '--execute' not in sys.argv
    
    if dry_run:
        print("üîç DRY RUN MODE - No files will be modified")
        print("   Use --execute to apply changes")
    else:
        print("‚ö†Ô∏è  EXECUTE MODE - Files will be modified!")
    
    # Files to convert
    files_to_convert = [
        'src/unified_shipstation_sync.py',
        'src/scheduled_xml_import.py',
        'src/scheduled_shipstation_upload.py',
        'src/scheduled_cleanup.py',
        'src/weekly_reporter.py',
        'src/shipstation_units_refresher.py',
        'app.py',
    ]
    
    for filepath in files_to_convert:
        path = Path(filepath)
        if path.exists():
            convert_file(path, dry_run)
        else:
            print(f"  ‚ùå File not found: {filepath}")
    
    if dry_run:
        print("\n‚úÖ Dry run complete - review changes above")
        print("   Run with --execute to apply changes")

if __name__ == '__main__':
    main()
```

### 4.4 Manual Code Review Checklist

After automated conversion, manually review:

- [ ] Exception handling (psycopg2.Error vs sqlite3.Error)
- [ ] `BEGIN IMMEDIATE` ‚Üí `BEGIN` (PostgreSQL doesn't support IMMEDIATE)
- [ ] Column type handling (INTEGER vs BIGINT)
- [ ] Boolean handling (0/1 vs TRUE/FALSE)
- [ ] AUTOINCREMENT removed from queries (schema only)
- [ ] All PRAGMA statements removed
- [ ] Transaction patterns match PostgreSQL best practices

---

## Phase 5: Data Freeze & Migration

### 5.1 Pre-Migration Data Freeze (CRITICAL)

**This prevents data loss during migration**

```bash
# migration/scripts/freeze_for_migration.sh
#!/bin/bash
set -e

echo "üîí FREEZING SYSTEM FOR MIGRATION"
echo "================================="

# 1. Disable all workflows
echo "‚è∏Ô∏è  Disabling all workflows..."
sqlite3 ora.db "UPDATE workflow_controls SET enabled = 0;"

# 2. Wait for in-flight operations
echo "‚è≥ Waiting 60 seconds for in-flight operations..."
sleep 60

# 3. Verify no pending uploads
PENDING=$(sqlite3 ora.db "SELECT COUNT(*) FROM orders_inbox WHERE uploaded_to_shipstation = 0;")
echo "üìä Pending uploads: $PENDING"

if [ "$PENDING" -gt 0 ]; then
    echo "‚ö†Ô∏è  WARNING: $PENDING orders pending upload"
    read -p "Continue anyway? (yes/no): " CONFIRM
    if [ "$CONFIRM" != "yes" ]; then
        echo "‚ùå Migration aborted"
        exit 1
    fi
fi

# 4. Final backup
echo "üì¶ Creating final pre-migration backup..."
cp ora.db migration/backups/ora_final_before_postgres_$(date +%Y%m%d_%H%M%S).db

# 5. Document freeze timestamp
date > migration/freeze_timestamp.txt

echo "================================="
echo "‚úÖ SYSTEM FROZEN - Safe to migrate"
echo "   No new data will be written"
```

**Execute freeze:**
```bash
bash migration/scripts/freeze_for_migration.sh
```

### 5.2 Data Migration with Validation

```python
# migration/scripts/postgres/02_migrate_data.py
"""Data migration with comprehensive validation"""

import sqlite3
import psycopg2
from psycopg2.extras import execute_values
import os
from datetime import datetime

class MigrationValidator:
    def __init__(self):
        self.errors = []
        self.warnings = []
    
    def add_error(self, msg):
        self.errors.append(msg)
        print(f"  ‚ùå ERROR: {msg}")
    
    def add_warning(self, msg):
        self.warnings.append(msg)
        print(f"  ‚ö†Ô∏è  WARNING: {msg}")
    
    def has_errors(self):
        return len(self.errors) > 0
    
    def report(self):
        if self.errors:
            print(f"\n‚ùå {len(self.errors)} ERRORS found - migration FAILED")
            return False
        if self.warnings:
            print(f"\n‚ö†Ô∏è  {len(self.warnings)} warnings - review recommended")
        else:
            print(f"\n‚úÖ No errors or warnings")
        return True

def get_sqlite_conn():
    return sqlite3.connect('migration/backups/ora_final_before_postgres_*.db')

def get_postgres_conn():
    return psycopg2.connect(os.environ['DATABASE_URL'])

def migrate_table(table_name, sqlite_conn, pg_conn, transform_fn=None, validator=None):
    """Migrate table with validation"""
    print(f"\nüì¶ Migrating {table_name}...")
    
    # Fetch from SQLite
    sqlite_cur = sqlite_conn.cursor()
    sqlite_cur.execute(f"SELECT * FROM {table_name}")
    rows = sqlite_cur.fetchall()
    columns = [desc[0] for desc in sqlite_cur.description]
    
    if not rows:
        print(f"   ‚ÑπÔ∏è  No data in {table_name}")
        return
    
    # Transform if needed
    if transform_fn:
        rows = [transform_fn(dict(zip(columns, row))) for row in rows]
        columns = list(rows[0].keys()) if rows else columns
    
    # Insert into PostgreSQL
    pg_cur = pg_conn.cursor()
    
    try:
        # Build parameterized insert
        placeholders = ','.join(['%s'] * len(columns))
        insert_sql = f"""
            INSERT INTO {table_name} ({','.join(columns)}) 
            VALUES %s
            ON CONFLICT DO NOTHING
        """
        
        # Batch insert
        values = [[row[col] if isinstance(row, dict) else row[i] 
                  for i, col in enumerate(columns)] for row in rows]
        
        execute_values(pg_cur, insert_sql, values, template=f"({placeholders})")
        pg_conn.commit()
        
        # Validate row count
        pg_cur.execute(f"SELECT COUNT(*) FROM {table_name}")
        pg_count = pg_cur.fetchone()[0]
        
        if pg_count == len(rows):
            print(f"   ‚úÖ Migrated {pg_count} rows")
        else:
            validator.add_error(f"{table_name}: Row count mismatch (SQLite={len(rows)}, PG={pg_count})")
        
    except Exception as e:
        validator.add_error(f"{table_name}: Migration failed - {e}")
        pg_conn.rollback()

def boolean_transform(row):
    """Transform SQLite integers to PostgreSQL booleans"""
    bool_fields = ['enabled', 'uploaded_to_shipstation', 'is_active']
    for key in bool_fields:
        if key in row and row[key] is not None:
            row[key] = bool(row[key])
    return row

def main():
    print("üöÄ MIGRATING DATA: SQLite ‚Üí PostgreSQL")
    print("=" * 50)
    
    validator = MigrationValidator()
    
    try:
        sqlite_conn = get_sqlite_conn()
        pg_conn = get_postgres_conn()
        
        # Migration order (respects foreign keys)
        migration_plan = [
            ('configuration_params', None),
            ('workflow_controls', boolean_transform),
            ('bundle_skus', None),
            ('bundle_components', None),
            ('sku_lot', boolean_transform),
            ('inventory_current', None),
            ('inventory_transactions', None),
            ('shipped_orders', None),  # Parent before children
            ('shipped_items', None),
            ('orders_inbox', boolean_transform),
            ('system_kpis', None),
            ('shipping_violations', None),
        ]
        
        for table, transform in migration_plan:
            migrate_table(table, sqlite_conn, pg_conn, transform, validator)
        
        # Final validation
        print("\n" + "=" * 50)
        if validator.report():
            print("‚úÖ DATA MIGRATION SUCCESSFUL")
            return 0
        else:
            print("‚ùå DATA MIGRATION FAILED")
            return 1
            
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        return 1
    finally:
        if 'sqlite_conn' in locals():
            sqlite_conn.close()
        if 'pg_conn' in locals():
            pg_conn.close()

if __name__ == '__main__':
    exit(main())
```

### 5.3 Comprehensive Validation

```python
# migration/scripts/postgres/03_verify_migration.py
"""Comprehensive migration validation"""

import sqlite3
import psycopg2
import os

class ValidationReport:
    def __init__(self):
        self.tests_run = 0
        self.tests_passed = 0
        self.tests_failed = 0
        self.failures = []
    
    def test(self, name, condition, details=""):
        self.tests_run += 1
        if condition:
            self.tests_passed += 1
            print(f"  ‚úÖ {name}")
        else:
            self.tests_failed += 1
            self.failures.append((name, details))
            print(f"  ‚ùå {name}: {details}")
    
    def summary(self):
        print(f"\n{'='*60}")
        print(f"Validation Summary:")
        print(f"  Tests Run: {self.tests_run}")
        print(f"  Passed: {self.tests_passed}")
        print(f"  Failed: {self.tests_failed}")
        print(f"{'='*60}")
        
        if self.tests_failed > 0:
            print("\n‚ùå VALIDATION FAILED - Do not proceed to deployment")
            return False
        else:
            print("\n‚úÖ VALIDATION PASSED - Safe to deploy")
            return True

def verify():
    report = ValidationReport()
    
    sqlite_conn = sqlite3.connect('migration/backups/ora_final_before_postgres_*.db')
    pg_conn = psycopg2.connect(os.environ['DATABASE_URL'])
    
    tables = [
        'configuration_params', 'workflow_controls', 'bundle_skus',
        'bundle_components', 'sku_lot', 'orders_inbox', 'shipped_orders',
        'shipped_items', 'inventory_current', 'inventory_transactions',
        'system_kpis', 'shipping_violations'
    ]
    
    print("üîç MIGRATION VALIDATION")
    print("=" * 60)
    
    # 1. Row count validation
    print("\n1Ô∏è‚É£  Row Count Validation:")
    for table in tables:
        sqlite_cur = sqlite_conn.cursor()
        sqlite_cur.execute(f"SELECT COUNT(*) FROM {table}")
        sqlite_count = sqlite_cur.fetchone()[0]
        
        pg_cur = pg_conn.cursor()
        pg_cur.execute(f"SELECT COUNT(*) FROM {table}")
        pg_count = pg_cur.fetchone()[0]
        
        report.test(
            f"{table}: {pg_count} rows",
            sqlite_count == pg_count,
            f"Mismatch: SQLite={sqlite_count}, PG={pg_count}"
        )
    
    # 2. Foreign key integrity
    print("\n2Ô∏è‚É£  Foreign Key Integrity:")
    pg_cur = pg_conn.cursor()
    
    pg_cur.execute("""
        SELECT COUNT(*) FROM shipped_items si
        LEFT JOIN shipped_orders so ON si.order_number = so.order_number
        WHERE so.order_number IS NULL
    """)
    orphaned = pg_cur.fetchone()[0]
    report.test("Shipped items have valid orders", orphaned == 0, f"{orphaned} orphaned items")
    
    pg_cur.execute("""
        SELECT COUNT(*) FROM bundle_components bc
        LEFT JOIN bundle_skus bs ON bc.bundle_sku = bs.bundle_sku
        WHERE bs.bundle_sku IS NULL
    """)
    orphaned = pg_cur.fetchone()[0]
    report.test("Bundle components have valid bundles", orphaned == 0, f"{orphaned} orphaned components")
    
    # 3. Data consistency
    print("\n3Ô∏è‚É£  Data Consistency:")
    
    # Check for nulls in required fields
    pg_cur.execute("SELECT COUNT(*) FROM shipped_orders WHERE order_number IS NULL")
    nulls = pg_cur.fetchone()[0]
    report.test("No null order numbers", nulls == 0, f"{nulls} null order numbers")
    
    # Check for invalid dates
    pg_cur.execute("SELECT COUNT(*) FROM shipped_orders WHERE ship_date > CURRENT_TIMESTAMP")
    future = pg_cur.fetchone()[0]
    report.test("No future ship dates", future == 0, f"{future} future dates")
    
    # 4. Index validation
    print("\n4Ô∏è‚É£  Index Validation:")
    pg_cur.execute("""
        SELECT COUNT(*) FROM pg_indexes 
        WHERE schemaname = 'public' AND tablename IN %s
    """, (tuple(tables),))
    index_count = pg_cur.fetchone()[0]
    report.test("Indexes created", index_count >= 10, f"Only {index_count} indexes found")
    
    sqlite_conn.close()
    pg_conn.close()
    
    return report.summary()

if __name__ == '__main__':
    success = verify()
    exit(0 if success else 1)
```

---

## Phase 6: Comprehensive Testing

### 6.1 Dual-Database Testing (Critical!)

**Run both SQLite and PostgreSQL simultaneously to compare results**

```python
# migration/scripts/test_dual_database.py
"""Run workflows against both databases and compare results"""

import os
import subprocess
import json

def run_workflow(workflow_script, use_postgres):
    """Run workflow and capture output"""
    env = os.environ.copy()
    env['USE_POSTGRES'] = 'true' if use_postgres else 'false'
    
    result = subprocess.run(
        ['python', workflow_script],
        env=env,
        capture_output=True,
        text=True,
        timeout=60
    )
    
    return result.returncode, result.stdout, result.stderr

def test_workflow(workflow_name, workflow_script):
    """Test workflow on both databases"""
    print(f"\nüß™ Testing: {workflow_name}")
    print("-" * 50)
    
    # Run on SQLite
    print("  üìä Running on SQLite...")
    sqlite_code, sqlite_out, sqlite_err = run_workflow(workflow_script, False)
    
    # Run on PostgreSQL
    print("  üìä Running on PostgreSQL...")
    pg_code, pg_out, pg_err = run_workflow(workflow_script, True)
    
    # Compare results
    if sqlite_code == pg_code == 0:
        print("  ‚úÖ Both succeeded")
    elif sqlite_code == 0 and pg_code != 0:
        print(f"  ‚ùå PostgreSQL failed: {pg_err}")
        return False
    elif sqlite_code != 0 and pg_code == 0:
        print(f"  ‚ö†Ô∏è  SQLite failed but PostgreSQL succeeded")
    else:
        print(f"  ‚ùå Both failed")
        return False
    
    return True

def main():
    print("üî¨ DUAL-DATABASE TESTING")
    print("=" * 50)
    
    workflows = [
        ('Unified ShipStation Sync', 'src/unified_shipstation_sync.py'),
        ('XML Import', 'src/scheduled_xml_import.py'),
        ('ShipStation Upload', 'src/scheduled_shipstation_upload.py'),
        ('Weekly Reporter', 'src/weekly_reporter.py'),
    ]
    
    results = []
    for name, script in workflows:
        results.append(test_workflow(name, script))
    
    print("\n" + "=" * 50)
    if all(results):
        print("‚úÖ ALL DUAL-DATABASE TESTS PASSED")
        return 0
    else:
        print("‚ùå SOME DUAL-DATABASE TESTS FAILED")
        return 1

if __name__ == '__main__':
    exit(main())
```

### 6.2 Load Testing

```python
# migration/scripts/test_load.py
"""Test PostgreSQL performance under load"""

import psycopg2
import os
import time
from concurrent.futures import ThreadPoolExecutor

def test_concurrent_reads():
    """Test concurrent SELECT queries"""
    def read_query():
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM shipped_orders WHERE ship_date >= CURRENT_DATE - INTERVAL '30 days'")
        result = cur.fetchone()
        conn.close()
        return result
    
    print("üîÑ Testing concurrent reads...")
    start = time.time()
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(read_query) for _ in range(100)]
        results = [f.result() for f in futures]
    
    duration = time.time() - start
    print(f"  ‚úÖ 100 queries in {duration:.2f}s ({100/duration:.1f} qps)")
    
    return duration < 5.0  # Should complete in < 5 seconds

def test_concurrent_writes():
    """Test concurrent INSERT/UPDATE queries"""
    def write_query(i):
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO system_kpis (metric_name, metric_value)
            VALUES (%s, %s)
            ON CONFLICT (metric_name) DO UPDATE SET metric_value = EXCLUDED.metric_value
        """, (f'test_metric_{i}', str(i)))
        conn.commit()
        conn.close()
    
    print("‚úèÔ∏è  Testing concurrent writes...")
    start = time.time()
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(write_query, i) for i in range(50)]
        [f.result() for f in futures]
    
    duration = time.time() - start
    print(f"  ‚úÖ 50 writes in {duration:.2f}s ({50/duration:.1f} wps)")
    
    # Cleanup
    conn = psycopg2.connect(os.environ['DATABASE_URL'])
    cur = conn.cursor()
    cur.execute("DELETE FROM system_kpis WHERE metric_name LIKE 'test_metric_%'")
    conn.commit()
    conn.close()
    
    return duration < 10.0  # Should complete in < 10 seconds

def main():
    print("‚ö° LOAD TESTING")
    print("=" * 50)
    
    tests = [
        ("Concurrent Reads", test_concurrent_reads),
        ("Concurrent Writes", test_concurrent_writes),
    ]
    
    results = []
    for name, test_fn in tests:
        try:
            results.append(test_fn())
        except Exception as e:
            print(f"  ‚ùå {name} failed: {e}")
            results.append(False)
    
    print("\n" + "=" * 50)
    if all(results):
        print("‚úÖ LOAD TESTS PASSED")
        return 0
    else:
        print("‚ùå LOAD TESTS FAILED")
        return 1

if __name__ == '__main__':
    exit(main())
```

---

## Phase 7: Deployment Configuration

### 7.1 Replit Deployment Setup

**Update Replit Configuration:**

```toml
# .replit
run = "bash start_all.sh"
modules = ["python-3.11", "postgresql"]

[env]
USE_POSTGRES = "true"
PYTHONUNBUFFERED = "1"

[deployment]
run = ["bash", "start_all.sh"]
deploymentTarget = "cloudrun"
```

### 7.2 Secrets Configuration

**In Replit Secrets pane:**
1. Verify `DATABASE_URL` exists (auto-created)
2. Add `USE_POSTGRES=true`
3. Ensure secrets are available to deployments (check "Available to deployments")

**Verify secrets:**
```bash
# Test in Replit shell
echo $DATABASE_URL
echo $USE_POSTGRES
```

### 7.3 Update Startup Script

```bash
# start_all.sh
#!/bin/bash

echo "üöÄ Starting ORA Automation (PostgreSQL Mode)"
echo "============================================"

# Verify PostgreSQL connection
if [ "$USE_POSTGRES" = "true" ]; then
    echo "‚úÖ PostgreSQL mode enabled"
    psql $DATABASE_URL -c "SELECT 1;" > /dev/null 2>&1 || {
        echo "‚ùå Cannot connect to PostgreSQL!"
        exit 1
    }
    echo "‚úÖ PostgreSQL connection verified"
else
    echo "‚ö†Ô∏è  Running in SQLite mode"
fi

# Start workflows
echo "Starting XML import..."
python src/scheduled_xml_import.py &

echo "Starting ShipStation upload..."
python src/scheduled_shipstation_upload.py &

echo "Starting Unified ShipStation Sync..."
python src/unified_shipstation_sync.py &

echo "Starting cleanup..."
python src/scheduled_cleanup.py &

echo "Starting weekly reporter..."
python src/weekly_reporter.py &

echo "Starting dashboard..."
python app.py
```

---

## Phase 8: Production Cutover

### 8.1 Pre-Cutover Checklist

**MANDATORY GO/NO-GO CHECKLIST:**

- [ ] All backups verified and downloaded
- [ ] PostgreSQL connection tested
- [ ] Schema created successfully
- [ ] Data migrated and validated (row counts match)
- [ ] Code changes completed and tested
- [ ] Dual-database tests passed
- [ ] Load tests passed
- [ ] Rollback script tested
- [ ] Replit deployment configured
- [ ] Secrets configured for deployment
- [ ] Team notified of cutover window
- [ ] Monitoring dashboard ready

**If ANY item fails, DO NOT proceed. Fix and retest.**

### 8.2 Cutover Execution Plan

**Maintenance Window: 2 hours**

```bash
# Cutover Execution Script
# migration/scripts/cutover.sh

#!/bin/bash
set -e

echo "üöÄ PRODUCTION CUTOVER TO POSTGRESQL"
echo "==================================="

# Checkpoint 1: Verify freeze
echo "Checkpoint 1: Verify system is frozen"
FROZEN=$(sqlite3 ora.db "SELECT COUNT(*) FROM workflow_controls WHERE enabled = 1;")
if [ "$FROZEN" -gt 0 ]; then
    echo "‚ùå ERROR: Workflows still enabled!"
    exit 1
fi
echo "‚úÖ System frozen confirmed"

# Checkpoint 2: Final validation
echo "Checkpoint 2: Run final validation"
python migration/scripts/postgres/03_verify_migration.py || {
    echo "‚ùå Validation failed - ABORTING"
    exit 1
}
echo "‚úÖ Validation passed"

# Checkpoint 3: Update environment
echo "Checkpoint 3: Enable PostgreSQL"
export USE_POSTGRES=true
echo "‚úÖ Environment updated"

# Checkpoint 4: Test single workflow (canary)
echo "Checkpoint 4: Canary test - weekly reporter"
python src/weekly_reporter.py || {
    echo "‚ùå Canary test failed - ROLLING BACK"
    bash migration/scripts/rollback/01_rollback_to_sqlite.sh
    exit 1
}
echo "‚úÖ Canary test passed"

# Checkpoint 5: Enable all workflows in PostgreSQL
echo "Checkpoint 5: Enable all workflows"
psql $DATABASE_URL -c "UPDATE workflow_controls SET enabled = TRUE;"
echo "‚úÖ Workflows enabled in PostgreSQL"

# Checkpoint 6: Republish deployment
echo "Checkpoint 6: Deploy to production"
echo "üìã MANUAL STEP: Click 'Republish' in Replit UI"
read -p "Press Enter after republishing..."

# Checkpoint 7: Verify deployment
echo "Checkpoint 7: Verify deployment"
sleep 30  # Wait for deployment
curl -f http://localhost:5000/api/dashboard_stats || {
    echo "‚ùå Dashboard not responding - CHECK DEPLOYMENT"
    exit 1
}
echo "‚úÖ Dashboard responding"

# Checkpoint 8: Monitor logs
echo "Checkpoint 8: Monitor initial operations"
echo "üîç Watching logs for 5 minutes..."
timeout 300 tail -f logs/*.log | grep -i error || true

echo "==================================="
echo "‚úÖ CUTOVER COMPLETE"
echo "   System running on PostgreSQL"
echo "   Monitor closely for 24 hours"
```

### 8.3 Abort Criteria

**Trigger rollback immediately if:**
- ‚ùå Any workflow fails after 3 retries
- ‚ùå Dashboard returns 500 errors
- ‚ùå Database connection failures
- ‚ùå Row count mismatches detected
- ‚ùå Foreign key violations
- ‚ùå Performance degradation > 50%
- ‚ùå Any data corruption detected

**Rollback command:**
```bash
bash migration/scripts/rollback/01_rollback_to_sqlite.sh
```

---

## Phase 9: Post-Migration Monitoring

### 9.1 First 24 Hours (Critical Monitoring)

**Automated Monitoring Script:**

```python
# migration/scripts/monitor_postgres.py
"""Post-migration monitoring"""

import psycopg2
import os
import time
from datetime import datetime

def check_connection_pool():
    """Monitor connection pool usage"""
    conn = psycopg2.connect(os.environ['DATABASE_URL'])
    cur = conn.cursor()
    
    cur.execute("""
        SELECT count(*) as active_connections
        FROM pg_stat_activity
        WHERE datname = current_database()
    """)
    active = cur.fetchone()[0]
    
    print(f"  Active connections: {active}")
    
    if active > 15:
        print(f"  ‚ö†Ô∏è  High connection count!")
    
    conn.close()
    return active < 20

def check_slow_queries():
    """Monitor slow queries"""
    conn = psycopg2.connect(os.environ['DATABASE_URL'])
    cur = conn.cursor()
    
    cur.execute("""
        SELECT query, calls, mean_exec_time, max_exec_time
        FROM pg_stat_statements
        WHERE mean_exec_time > 100
        ORDER BY mean_exec_time DESC
        LIMIT 5
    """)
    
    slow_queries = cur.fetchall()
    
    if slow_queries:
        print(f"  ‚ö†Ô∏è  {len(slow_queries)} slow queries detected")
        for query, calls, mean, max_time in slow_queries:
            print(f"    {query[:50]}... (mean: {mean:.2f}ms, max: {max_time:.2f}ms)")
    else:
        print(f"  ‚úÖ No slow queries")
    
    conn.close()
    return len(slow_queries) < 5

def check_workflow_status():
    """Monitor workflow execution"""
    conn = psycopg2.connect(os.environ['DATABASE_URL'])
    cur = conn.cursor()
    
    cur.execute("SELECT workflow_name, enabled FROM workflow_controls")
    workflows = cur.fetchall()
    
    enabled_count = sum(1 for _, enabled in workflows if enabled)
    print(f"  Workflows: {enabled_count}/{len(workflows)} enabled")
    
    conn.close()
    return True

def monitor_loop():
    """Continuous monitoring loop"""
    print("üîç POST-MIGRATION MONITORING")
    print("=" * 50)
    
    iteration = 0
    while True:
        iteration += 1
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        print(f"\n[{timestamp}] Check #{iteration}")
        print("-" * 50)
        
        try:
            check_connection_pool()
            check_slow_queries()
            check_workflow_status()
        except Exception as e:
            print(f"  ‚ùå Monitoring error: {e}")
        
        time.sleep(300)  # Check every 5 minutes

if __name__ == '__main__':
    monitor_loop()
```

**Run monitoring:**
```bash
# In separate terminal
python migration/scripts/monitor_postgres.py
```

### 9.2 Week 1 Checklist

**Daily Tasks:**
- [ ] Review monitoring dashboard
- [ ] Check slow query log
- [ ] Verify workflow success rate
- [ ] Review error logs
- [ ] Check database size growth
- [ ] Verify backup retention

**Metrics to Track:**
- Average query time (should be < 100ms)
- Connection pool usage (should be < 80%)
- Workflow success rate (should be > 99%)
- Error rate (should be < 1%)

### 9.3 Optimization

```sql
-- Analyze tables for query optimization
ANALYZE shipped_orders;
ANALYZE shipped_items;
ANALYZE orders_inbox;

-- Check for missing indexes
SELECT schemaname, tablename, attname, n_distinct, correlation
FROM pg_stats
WHERE schemaname = 'public'
ORDER BY abs(correlation) DESC;

-- Vacuum to reclaim space
VACUUM ANALYZE;
```

---

## Automated Rollback Procedures

### Complete Rollback Script

```bash
# migration/scripts/rollback/01_rollback_to_sqlite.sh
#!/bin/bash
set -e

echo "üîÑ EMERGENCY ROLLBACK TO SQLITE"
echo "================================"

ROLLBACK_LOG="migration/logs/rollback_$(date +%Y%m%d_%H%M%S).log"

log() {
    echo "$1" | tee -a "$ROLLBACK_LOG"
}

log "Starting rollback at $(date)"

# Step 1: Disable PostgreSQL workflows
log "Step 1: Disabling PostgreSQL workflows..."
if [ -n "$DATABASE_URL" ]; then
    psql $DATABASE_URL -c "UPDATE workflow_controls SET enabled = FALSE;" || log "Warning: Could not disable PG workflows"
fi

# Step 2: Restore SQLite backup
log "Step 2: Restoring SQLite backup..."
BACKUP=$(ls -t migration/backups/ora_final_before_postgres_*.db 2>/dev/null | head -1)

if [ -z "$BACKUP" ]; then
    log "ERROR: No SQLite backup found!"
    exit 1
fi

cp "$BACKUP" ora.db
log "Restored: $BACKUP"

# Step 3: Verify SQLite backup
log "Step 3: Verifying SQLite backup..."
ROW_COUNT=$(sqlite3 ora.db "SELECT COUNT(*) FROM shipped_orders;" 2>/dev/null || echo "0")
log "SQLite row count: $ROW_COUNT"

if [ "$ROW_COUNT" -eq 0 ]; then
    log "ERROR: SQLite backup appears empty!"
    exit 1
fi

# Step 4: Revert code to SQLite version
log "Step 4: Reverting code to pre-migration state..."
git checkout pre-postgres-migration-* 2>/dev/null || {
    log "Warning: Could not find pre-migration tag, using last commit"
    git checkout HEAD~1
}

# Step 5: Update environment
log "Step 5: Updating environment to SQLite..."
export USE_POSTGRES=false
unset DATABASE_URL

# Step 6: Restart workflows
log "Step 6: Restarting workflows with SQLite..."
pkill -f "python src/" || true
sleep 5

# Start workflows
python src/unified_shipstation_sync.py &
python src/scheduled_xml_import.py &
python src/scheduled_shipstation_upload.py &
python src/weekly_reporter.py &
python app.py &

# Step 7: Verify rollback
log "Step 7: Verifying rollback..."
sleep 10

if curl -f http://localhost:5000/api/dashboard_stats > /dev/null 2>&1; then
    log "‚úÖ Dashboard responding"
else
    log "‚ö†Ô∏è  Dashboard not responding - check manually"
fi

log "================================"
log "‚úÖ ROLLBACK COMPLETE"
log "   System restored to SQLite"
log "   Review log: $ROLLBACK_LOG"
log "   Ended at $(date)"

# Enable SQLite workflows
sqlite3 ora.db "UPDATE workflow_controls SET enabled = 1;"

echo ""
echo "üìã NEXT STEPS:"
echo "1. Verify dashboard at http://localhost:5000"
echo "2. Check workflow logs"
echo "3. Review rollback log: $ROLLBACK_LOG"
echo "4. Document rollback reason"
```

**Test rollback (dry-run):**
```bash
# Verify script syntax
bash -n migration/scripts/rollback/01_rollback_to_sqlite.sh

# Run in test mode
USE_POSTGRES=false bash migration/scripts/rollback/01_rollback_to_sqlite.sh --dry-run
```

---

## Troubleshooting Guide

### Issue 1: Connection Pool Exhausted

**Symptom:** `psycopg2.pool.PoolError: connection pool exhausted`

**Diagnosis:**
```python
# Check active connections
import psycopg2
conn = psycopg2.connect(os.environ['DATABASE_URL'])
cur = conn.cursor()
cur.execute("SELECT count(*) FROM pg_stat_activity WHERE datname = current_database()")
print(f"Active connections: {cur.fetchone()[0]}")
```

**Resolution:**
```python
# Increase pool size in pg_utils.py
connection_pool = pool.SimpleConnectionPool(
    minconn=2,
    maxconn=30,  # Increase from 20
    dsn=os.environ.get('DATABASE_URL')
)
```

### Issue 2: Slow Queries

**Symptom:** Dashboard/workflows slow after migration

**Diagnosis:**
```sql
-- Find slow queries
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
WHERE mean_exec_time > 100
ORDER BY mean_exec_time DESC
LIMIT 10;
```

**Resolution:**
```sql
-- Add missing index
CREATE INDEX idx_custom ON table_name(column_name);

-- Analyze table
ANALYZE table_name;
```

### Issue 3: Data Type Errors

**Symptom:** `psycopg2.ProgrammingError: column is of type boolean but expression is of type integer`

**Resolution:**
```python
# Update transformation function
def boolean_transform(row):
    bool_fields = ['enabled', 'uploaded_to_shipstation', 'is_active']
    for key in bool_fields:
        if key in row and row[key] is not None:
            row[key] = bool(int(row[key]))  # Explicit conversion
    return row
```

### Issue 4: Foreign Key Violations

**Symptom:** `psycopg2.IntegrityError: violates foreign key constraint`

**Diagnosis:**
```sql
-- Check for orphaned records
SELECT COUNT(*) FROM shipped_items si
LEFT JOIN shipped_orders so ON si.order_number = so.order_number
WHERE so.order_number IS NULL;
```

**Resolution:**
```python
# Adjust migration order - ensure parent tables migrate first
migration_plan = [
    ('shipped_orders', None),  # Parent first
    ('shipped_items', None),   # Then children
]
```

---

## Migration Timeline (Revised)

| Phase | Duration | Key Activities | Owner |
|-------|----------|---------------|-------|
| **Preparation** | 2 hours | Backups, rollback scripts, code inventory | Engineer |
| **Environment Setup** | 1 hour | Dependencies, Replit config, PostgreSQL provisioning | Engineer |
| **Schema Migration** | 1 hour | Create tables, indexes, constraints | Engineer |
| **Code Migration** | 8-10 hours | Update 15+ files, SQL syntax conversion | Engineer |
| **Data Freeze** | 1 hour | Freeze workflows, verify pending operations | Engineer |
| **Data Migration** | 2 hours | Migrate data, validate row counts | Engineer |
| **Testing** | 6-8 hours | Dual-database, load testing, validation | Engineer + QA |
| **Deployment Config** | 2 hours | Replit setup, secrets, startup scripts | Engineer |
| **Cutover** | 2 hours | Production deployment, monitoring | Engineer + Lead |
| **Post-Migration** | 4 hours | 24-hour monitoring, optimization | Engineer |
| **TOTAL** | **29-37 hours** | Complete migration with contingency | Team |

**Recommended Schedule:**
- **Day 1 (8 hours):** Preparation + Environment + Schema + Code Migration (start)
- **Day 2 (8 hours):** Code Migration (complete) + Data Freeze + Data Migration
- **Day 3 (8 hours):** Testing (comprehensive validation)
- **Day 4 (6 hours):** Deployment Config + Cutover + Initial Monitoring
- **Day 5-7:** Post-migration monitoring and optimization

---

## Success Criteria

**Migration is successful when:**

‚úÖ **Technical Metrics:**
- All row counts match SQLite source (100% accuracy)
- Zero foreign key violations
- Zero data corruption
- Database queries < 100ms average
- Connection pool < 80% utilized
- Workflow success rate > 99%
- Zero deployment data loss

‚úÖ **Operational Metrics:**
- Dashboard displays real-time data
- All workflows running without errors
- Orders processing correctly
- Inventory tracking accurate
- Republish does NOT cause data loss
- Rollback procedure tested and verified

‚úÖ **Business Metrics:**
- Zero order processing delays
- No revenue impact
- Team confident in new system
- Production stable for 7 days

---

## Appendix

### A. Environment Variables

```bash
# Replit-managed (auto-created)
DATABASE_URL=postgresql://user:pass@host:5432/dbname
PGHOST=host
PGPORT=5432
PGUSER=user
PGPASSWORD=password
PGDATABASE=dbname

# Application-managed
USE_POSTGRES=true
DEV_MODE=false
```

### B. Key SQL Syntax Differences

```sql
-- Placeholders
SQLite:    SELECT * FROM orders WHERE id = ?
PostgreSQL: SELECT * FROM orders WHERE id = %s

-- Current timestamp
SQLite:    DATETIME('now')
PostgreSQL: CURRENT_TIMESTAMP

-- Date arithmetic
SQLite:    DATE('now', '-7 days')
PostgreSQL: CURRENT_DATE - INTERVAL '7 days'

-- Auto-increment
SQLite:    id INTEGER PRIMARY KEY AUTOINCREMENT
PostgreSQL: id SERIAL PRIMARY KEY

-- Boolean
SQLite:    enabled INTEGER (0/1)
PostgreSQL: enabled BOOLEAN (TRUE/FALSE)
```

### C. Rollback Decision Tree

```
Is PostgreSQL working?
‚îú‚îÄ YES ‚Üí Continue monitoring
‚îî‚îÄ NO ‚Üí Check error type
    ‚îú‚îÄ Connection issues ‚Üí Verify DATABASE_URL
    ‚îú‚îÄ Data corruption ‚Üí IMMEDIATE ROLLBACK
    ‚îú‚îÄ Performance degradation ‚Üí Optimize or rollback
    ‚îî‚îÄ Foreign key violations ‚Üí IMMEDIATE ROLLBACK
```

---

**END OF MIGRATION PLAN v2.0**

*This plan has been reviewed and addresses all critical gaps identified in the initial assessment.*
